---
title: "Untitled"
format: pdf
editor: visual
---

```{r}
library(tree)
library(ISLR2)
```

##########labs #8.3.1 Fitting Classifcation Trees

```{r}
attach(Carseats)
High <- factor(ifelse(Sales <= 8, "No", "Yes"))
```

#merge High with the rest of the Carseats data.

```{r}
Carseats <- data.frame(Carseats, High)
```

#tree() function to ft a classifcation tree in order to predict High using all variables but Sales

```{r}
tree.carseats <- tree(High ~ . - Sales, Carseats)
summary(tree.carseats)
```

#Misclassification error rate (training error) = 0.09

```{r}
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```

```{r}
tree.carseats
```

```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train, ]
High.test <- High[-train]
tree.carseats <- tree(High ~ . - Sales, Carseats,
                      subset = train)
tree.pred <- predict(tree.carseats, Carseats.test,
                     type = "class") 
table(tree.pred, High.test)
```

```{r}
(104 + 50) / 200
```

#cv.tree() performs cross-validation in order to determine the optimal level of tree complexity #use the argument FUN = prune.misclass in order to indicate that we want the classifcation error rate to guide the cross-validation and pruning process

```{r}
set.seed(7)
cv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)

cv.carseats
```

```{r}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = "b") 
plot(cv.carseats$k, cv.carseats$dev, type = "b")
```

#number of terminal nodes of each tree considered (size) \# the value of the cost-complexity parameter used (k, which corresponds to α

#now apply the prune.misclass() function in order to prune the tree to obtain the nine-node tree.

```{r}
prune.carseats <- prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

```{r}
tree.pred <- predict(prune.carseats, Carseats.test,
                     type = "class") 
table(tree.pred, High.test)
```

```{r}
(97 + 58) / 200
```

# 77.5 % of the test observations are correctly classifed

#If we increase the value of best, we obtain a larger pruned tree with lower classifcation accuracy:

```{r}
prune.carseats <- prune.misclass(tree.carseats, best = 14)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred <- predict(prune.carseats, Carseats.test,
                     type = "class") 
table(tree.pred, High.test)
```

############8.3.2 Fitting Regression Trees

```{r}
set.seed(1)
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```

```{r}
plot(tree.boston)
text(tree.boston, pretty = 0)
```

#cv.tree() function to see whether pruning the tree will improve performance.

```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = "b")
```

#In this case, the most complex tree under consideration is selected by cross validation. \# However, if we wish to prune the tree, we could do so as follows, using the prune.tree() function:

```{r}
prune.boston <- prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```

#In keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.

```{r}
yhat <- predict(tree.boston, newdata = Boston[-train, ])
boston.test <- Boston[-train, "medv"] 
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
```

#In other words, the test set MSE associated with the regression tree is 35.29.The square root of the MSE is therefore around 5.941, indicating that this model leads to test predictions that are (on average) within approximately \$5,941 of the true median home value for the census tract.

#8.3.3 Bagging and Random Forests ########Bagging

```{r}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston,
                           subset = train, mtry = 12, #m = p Bagging
                           importance = TRUE)
bag.boston
```

```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
plot(yhat.bag, boston.test)
abline(0, 1)
mean((yhat.bag - boston.test)^2)
```

# test set MSE associated with the bagged regression tree is 23.42

# could change the number of trees grown by randomForest() using the ntree

```{r}
bag.boston <- randomForest(medv ~ ., data = Boston,
                           subset = train, mtry = 12, ntree = 25)
yhat.bag <- predict(bag.boston, newdata = Boston[-train, ])
mean((yhat.bag - boston.test)^2)
```

#############random forest #randomForest() uses p/3 variables when building a random forest of regression trees, and √p variables when building a random forest of classifcation trees.

```{r}
set.seed(1)
rf.boston <- randomForest(medv ~ ., data = Boston,
                          subset = train, mtry = 6, importance = TRUE)
yhat.rf <- predict(rf.boston, newdata = Boston[-train, ])
mean((yhat.rf - boston.test)^2)
```

# test set MSE is 20.07; this indicates that random forests yielded an improvement over bagging in this case.

```{r}
importance(rf.boston)
```

```{r}
varImpPlot(rf.boston)
```

######8.3.4 Boosting

```{r}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train, ], # default value is 0.001,lamda
                    distribution = "gaussian", n.trees = 5000, #a binary classifcation problem, we would use distribution = "bernoulli".
                    interaction.depth = 4)
```

```{r}
summary(boost.boston)
```

# In this case, as we might expect, median house prices are increasing with rm and decreasing with lstat.

```{r}
plot(boost.boston, i = "rm") 
plot(boost.boston, i = "lstat")
```

```{r}
yhat.boost <- predict(boost.boston,
                      newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

# test MSE obtained is 18.39: this is superior to the test MSE of random forests and bagging.

```{r}
boost.boston <- gbm(medv ~ ., data = Boston[train, ],
                    distribution = "gaussian", n.trees = 5000,
                    interaction.depth = 4, shrinkage = 0.2, verbose = F) #take λ = 0.2.
yhat.boost <- predict(boost.boston,
                      newdata = Boston[-train, ], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

#8.3.5 Bayesian Additive Regression Trees

```{r}
library(BART)
x <- Boston[, 1:12]
y <- Boston[, "medv"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
```

# compute the test error.

```{r}
yhat.bart <- bartfit$yhat.test.mean
mean((ytest - yhat.bart)^2)
```

#the test error of BART is lower than the test error of random forests and boosting

```{r}
ord <- order(bartfit$varcount.mean, decreasing = T)
bartfit$varcount.mean[ord]
```

##########exercises

#7.

```{r}
mtry_values <- c(3, 6, 12)  
ntree_values <- seq(1, 500, by = 10)  
test_errors <- matrix(NA, nrow = length(ntree_values), ncol = length(mtry_values))

for (m in 1:length(mtry_values)) {
  for (t in 1:length(ntree_values)) {
    rf.boston <- randomForest(medv ~ ., data = Boston,
                             subset = train, mtry = mtry_values[m], ntree = ntree_values[t])
    yhat.rf <-predict(rf.boston, newdata = Boston[-train, ])

    test_errors[t, m] <- mean((yhat.rf - boston.test)^2)
  }
}

colors <- c("red", "green", "blue")
plot(ntree_values, test_errors[, 1], type = "l", col = colors[1], lwd = 2,
     ylim = range(test_errors), xlab = "Number of Trees (ntree)", ylab = "Test MSE",
     main = "Test MSE for Random Forest with Varying mtry and ntree")
lines(ntree_values, test_errors[, 2], col = colors[2], lwd = 2)
lines(ntree_values, test_errors[, 3], col = colors[3], lwd = 2)
legend("topright", legend = paste("mtry =", mtry_values), col = colors, lty = 1, lwd = 2)

```

#8.

```{r}
detach(Carseats)
library(ISLR2)
data(Carseats)  
attach(Carseats)
Carseats
```

#a) Split the data set into a training set and a test set. #b) Fit a regression tree to the training set.

```{r}
set.seed(0)
train <- sample(1:nrow(Carseats), 0.7 * nrow(Carseats))

Carseats.test <- Carseats[-train,"Sales"]
tree.carseats <- tree(Sales ~ . - Sales, Carseats,
                      subset = train)

yhat <- predict(tree.carseats, newdata = Carseats[-train, ])
plot(yhat, Carseats.test)
abline(0, 1)
```

# test MSE obtain:4.208383

```{r}
mean((yhat - Carseats.test)^2)
```

#c)

```{r}
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
```

#14 node

```{r}
prune.carseats <- prune.tree(tree.carseats, best = 14)
plot(prune.carseats)
text(prune.carseats, pretty = 0)

tree.pred <- predict(prune.carseats, newdata = Carseats[-train, ])
mean((tree.pred - Carseats.test)^2)
```

# pruning the tree doesn't improve the test MSE

#d) Use the bagging approach in order to analyze this data

```{r}
set.seed(0)
bag.carseats <- randomForest(Sales ~ . - Sales, data = Carseats,
                             subset = train, mtry = 10, #m = p Bagging
                             importance = TRUE)

tree.pred <- predict(bag.carseats, newdata = Carseats[-train, ])
mean((tree.pred - Carseats.test)^2)
```

#CompPrice most important

```{r}
importance(bag.carseats)
```

#e) Use random forests to analyze this data. m = p\^0.5

```{r}
set.seed(0)
rf.carseats <- randomForest(Sales ~ . - Sales, data = Carseats,
                            subset = train, mtry = 3, #m = p^0.5
                            importance = TRUE)

tree.pred <- predict(rf.carseats, newdata = Carseats[-train, ])
mean((tree.pred - Carseats.test)^2)
```

#f)Bart

```{r}
x <- Carseats[, 2:11]
y <- Carseats[, "Sales"]
xtrain <- x[train, ]
ytrain <- y[train]
xtest <- x[-train, ]
ytest <- y[-train]
set.seed(1)
bartfit <- gbart(xtrain, ytrain, x.test = xtest)
```

```{r}
yhat.bart <- bartfit$yhat.test.mean
mean((ytest - yhat.bart)^2)
```

##test error of BART is smaller than that of random forest and bagging #9. involves the OJ data set which is part of the ISLR2 package. #Create a training set containing a random sample of 800 observations

```{r}
set.seed(0)
train <- sample(1:nrow(OJ), 800)

OJ.train <- OJ[train, ]
OJ.test <- OJ[-train, ]
Purchase.test <- OJ$Purchase[-train]  # Extract test target variable
```

#b) training error rate? How many terminal nodes does the tree have?

```{r}
tree.OJ <- tree(Purchase ~ . - Purchase, OJ,
                subset = train)
tree.pred <- predict(tree.OJ, OJ.train,
                     type = "class")
table(tree.pred, OJ$Purchase[train])
```

#training error =1- (416+257)/800 = 0.15875

#c) Type in the name of the tree object in order to get a detailed text output, Pick one of the terminal nodes, #d)

```{r}
tree.OJ
plot(tree.OJ)
text(tree.OJ, pretty = 0)
```

#predicted class at the root node is "CH", meaning the model would predict that customers are more likely to purchase CH (Citrus Hill) if no further splits are made.

#e) Predict the response on the test data

```{r}
tree.pred <- predict(tree.OJ, OJ.test,
                     type = "class")
table(tree.pred, Purchase.test)
```

#test error = 1 - (134+84)/270 = 0.1925926

#f)Use cross-validation to determine the optimal tree size.

```{r}
set.seed(0)
cv.OJ <- cv.tree(tree.OJ, FUN = prune.misclass)
names(cv.OJ)
par(mfrow = c(1, 2))
plot(cv.OJ$size, cv.OJ$dev, type = "b")
plot(cv.OJ$k, cv.OJ$dev, type = "b")
```

#best = 8 ##In this case, the most complex tree under consideration is selected by cross validation #i) create a pruned tree with five terminal nodes.

```{r}
prune.OJ <- prune.misclass(tree.OJ, best = 5)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
```

#j)

```{r}
tree.pred <- predict(prune.OJ, OJ.train,
                     type = "class")
table(tree.pred, OJ$Purchase[train])
```

training error= 1- (408+258)/800 = 0.1675 0.15875

# h) Compare the training error rates between the pruned

```{r}
tree.pred <- predict(prune.OJ, OJ.test,
                     type = "class")
table(tree.pred, Purchase.test)
```

##test error = 1 - (133+87)/270 = 0.1851852 #the pruned node =5 decrease the test error

#10.use boosting to predict Salary in the Hitters data set #a)

```{r}
library(ISLR2)
data(Hitters)  
attach(Hitters)
Hitters<- Hitters[!is.na(Hitters$Salary), ]
Hitters$Salary <- log(Hitters$Salary)
```

#b) Create a training set consisting of the frst 200 observations

```{r}
train <- 1:200

Hitters.train <- Hitters[train, ]
Hitters.test <- Hitters[-train, ]
Salary.train <- Hitters$Salary[train] 
Salary.test <- Hitters$Salary[-train]  
```



#c) Perform boosting on the training set with 1,000 trees


```{r}
set.seed(0)
learning_rates <- c(seq(0.0001, 0.001, 0.0001), seq(0.001, 0.31, 0.001))

train_mse <- numeric(length(learning_rates))
test_mse <- numeric(length(learning_rates))

# Loop over the learning rates and fit gradient boosting models
for (learning_rate in learning_rates) {
  boost.Hitters <- gbm(Salary ~ .,
                       data = Hitters[train, ], 
                       distribution = "gaussian", 
                       n.trees = 1000, 
                       shrinkage = learning_rate,  # Use learning_rate directly
                       interaction.depth = 3,
                       verbose = F)
  
  pred_train <- predict(boost.Hitters, newdata = Hitters[train, ], n.trees = 1000)
  pred_test <- predict(boost.Hitters, newdata = Hitters[-train, ], n.trees = 1000)
  
  train_mse[which(learning_rates == learning_rate)] <- mean((pred_train - Hitters$Salary[train])^2)
  test_mse[which(learning_rates == learning_rate)] <- mean((pred_test - Hitters$Salary[-train])^2)
}
```



#c)
```{r}
plot(learning_rates, train_mse, type = "b", col = "blue", 
     xlab = "Shrinkage (λ)", ylab = "Training Set MSE", 
     main = "Training Set MSE vs Shrinkage", pch = 19)
```
#d)
```{r}
plot(learning_rates, test_mse, type = "b", col = "red", 
     xlab = "Shrinkage (λ)", ylab = "Test Set MSE", 
     main = "Test Set MSE vs Shrinkage", pch = 19)
```

```{r}
optimal_index <- which.min(test_mse)
optimal_learning_rate <- learning_rates[optimal_index]
optimal_test_mse <- test_mse[optimal_index]

# Output the optimal learning rate and test MSE
cat("Optimal Learning Rate: ", optimal_learning_rate, "\n")
cat("Test MSE at Optimal Learning Rate: ", optimal_test_mse, "\n")
```
#e)
#OLS
```{r}
lm <- lm(Salary ~ ., data = Hitters, subset = train)
pred_test <- predict(lm, newdata = Hitters[-train, ])
test_mse <- mean((pred_test - Hitters$Salary[-train])^2)
cat("MSE for linear regression: ", test_mse, "\n")
```

## Lasso Regression
```{r}
library(glmnet)
set.seed(0)

x_train <- model.matrix(Salary ~ . - Salary, data = Hitters[train, ])
x_test<- model.matrix(Salary ~ . - Salary, data = Hitters[-train, ])
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x_train, Salary.train, alpha = 1,
                    lambda = grid)
cv.out <- cv.glmnet(x_train, Salary.train, alpha = 1)

bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam,
                      newx = x_test)
mean((lasso.pred - Salary.test)^2)
```

#both are much higher than boosting

#f) Which variables appear to be the most important predictors in the boosted model
```{r}
boost.Hitters <- gbm(Salary ~ .,
                       data = Hitters[train, ], 
                       distribution = "gaussian", 
                       n.trees = 1000, 
                       shrinkage = 0.047,  # Use learning_rate directly
                       interaction.depth = 3,
                       verbose = F)

summary(boost.Hitters)
```
#CAtBat
#g)bagging
```{r}
set.seed(0)
bag.Hitters <- randomForest(Salary ~ ., data = Hitters,
                            subset = train, mtry = 19, #m = p Bagging
                            importance = TRUE)
yhat.bag <- predict(bag.Hitters, newdata = Hitters[-train, ])
mean((yhat.bag - Salary.test)^2)
```

#11. This question uses the Caravan data set.

```{r}
train <- 1:1000

Caravan.train <- Caravan[train, ]
Caravan.test <- Caravan[-train, ]
Purchase.train <- Caravan$Purchase[train] 
Purchase.test <- Caravan$Purchase[-train]  
```

#Fit a boosting model
#PPERSAUT
#c)

```{r}
set.seed(0)
Caravan.train$Purchase_binary <- ifelse(Caravan.train$Purchase == 'Yes', 1, 0)
Caravan.test$Purchase_binary <- ifelse(Caravan.test$Purchase == 'Yes', 1, 0)


boost.Caravan <- gbm(Purchase_binary ~ . - Purchase, 
                     data = Caravan.train, 
                     distribution = "bernoulli",  # Binary classification
                     n.trees = 1000, 
                     shrinkage = 0.01, 
                     interaction.depth = 3, 
                     verbose = FALSE)
summary(boost.Caravan)

yhat.boost_prob <- predict(boost.Caravan, 
                           newdata = Caravan.test, 
                           n.trees = 1000, 
                           type = "response")  # 'response' for probabilities


yhat.boost <- ifelse(yhat.boost_prob > 0.2, 'Yes', 'No')


conf_matrix <- table(yhat.boost, Caravan.test$Purchase)
print(conf_matrix)

```
#What fraction of the people predicted to make a purchase do in fact make one? 37/(182+37) =0.169 >0.145.(KNN in lab4, but the training and testing set are different)
