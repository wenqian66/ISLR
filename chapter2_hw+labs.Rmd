---
title: "chapter2_hw+labs"
output:
  pdf_document: default
  html_document: default
  word_document: default
date: "2024-08-17"
---

```{r setup, include=FALSE}
library(tidyverse)# load the core tidyverse packages
library(dplyr) # load dplyr 
library(ggplot2)# gg plot
library(readxl) # need to load from tidyverse
library(readr) # to import data
library(png)
library(grid)
library(ggpubr)
library(ISLR2)
```


1.For each of parts (a) through (d), indicate whether we would generally
expect the performance of a fexible statistical learning method to be
better or worse than an infexible method. Justify your answer.
(a) The sample size n is extremely large, and the number of predictors p is small.
#fexible method better
(b) The number of predictors p is extremely large, and the number of observations n is small.
#infexible method better;overftting in highly fexible methods
(c) The relationship between the predictors and response is highly non-linear.
#a fexible statistical learning better
(d) The variance of the error terms, i.e. σ2 = Var(ϵ), is extremely high.
#infexible method better;  if a method has high variance then small changes in the training data can result in large changes in fˆ. In general, more fexible statistical methods have higher variance

2.Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

#regression; inference; n = 500; p = 3(profit, number of employees, industry salary)

(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price,and ten other variables.
#classification; prediction; n = 20; p = 13(price charged for the product, marketing budget, competition price,and ten other variables.)

(c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. 
Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

#regression; prediction; n = 52(weeks in 2012); p = 3(the % change in the US market, the % change in the British market, and the % change in the German market)

3. We now revisit the bias-variance decomposition.

(a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.

```{r}
rm(list=ls())
answer3 <- readPNG("bias-variance.png")
p1<-ggplot()+background_image(answer3)+theme_void()
p1
```

(b) Explain why each of the five curves has the shape displayed in part (a).

#As in the regression setting, the training error rate consistently declines as the fexibility increases.

#the test error exhibits a characteristic U-shape, declining at frst (with a minimum at approximately K = 10) before increasing again when the method becomes excessively flexible and overfits.

#The Bayes classifer produces the lowest possible test error rate

5. What are the advantages and disadvantages of a very flexible (versus
a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

#Generally, use more fexible methods, the variance will increase and the bias will decrease.
#when the true f is substantially non-linear, a more flexible approach would be preferred
#when the true f is close to linear, a less flexible approach would be preferred

6. Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a nonparametric approach)? What are its disadvantages?

#a parametric form for f simplifes the problem of estimating f(generally much easier to estimate a set of parameters)
#The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of f.

7. The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.

Suppose we wish to use this data set to make a prediction for Y when
X1 = X2 = X3 = 0 using K-nearest neighbors.
(a) Compute the Euclidean distance between each observation and
the test point, X1 = X2 = X3 = 0. # 3;2;3.162278;2.236068;1.414214;1.732051
```{r}
sqrt(((0-0)^2+(3-0)^2+(0-0)^2))
sqrt(((2-0)^2+(0-0)^2+(0-0)^2))
sqrt(((0-0)^2+(1-0)^2+(3-0)^2))
sqrt(((0-0)^2+(1-0)^2+(2-0)^2))
sqrt(((-1-0)^2+(0-0)^2+(1-0)^2))
sqrt(((1-0)^2+(1-0)^2+(1-0)^2))
```

(b) What is our prediction with K = 1? Why?
# Green (Closest:1.414214) (Obs:5)

(c) What is our prediction with K = 3? Why?
#Red (1.414214;1.732051;2)(Obs:2,5,6)(Red:2/3;Green:1/3) 

(d) If the Bayes decision boundary in this problem is highly nonlinear, then would we expect the best value for K to be large or small? Why?
#small; Since this problem is highly nonlinear, a smaller K results in an overly flexible decision boundary

8.This exercise relates to the College data set, which can be found in the file College.csv on the book website. It contains a number of variables for 777 different universities and colleges in the US.
```{r}
college <- read_csv("College.csv")
#change df to matrix to avoid "Error in `.rowNamesDF<-`(x, value = value)"
college <- as.matrix(college)
rownames(college) <- college[, 1]

#delete the first col
college <- college[, -1]
```

```{r}
#change back to df and the correct variables' types
college <- as.data.frame(college)
college[, 2:ncol(college)] <- lapply(college[, 2:ncol(college)], function(x) as.numeric(as.character(x)))
college$Private <- as.factor(college$Private)
summary(college)

pairs(college[,1:10])
plot(college$Private, college$Outstate, xlab = "Private", ylab = "Outstate")

```
```{r}
Elite <- rep("No", nrow(college))
Elite[college$Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(college, Elite)
# 78 elite universities
summary(college$Elite)
plot(college$Elite, college$Outstate, xlab = "Elite", ylab = "Outstate")
```
```{r}
par(mfrow = c(2, 2))
hist(college$Apps, col =5, xlab = "Apps", main = NULL)
hist(college$Accept, col =5, xlab = "Accept", main = NULL)
hist(college$Enroll, col =5, xlab = "Enroll", main = NULL)
hist(college$Top10perc, col =5, xlab = "Top10perc", main = NULL)
```

9.This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.
```{r}
Auto <- read_csv("Auto.csv")
Auto$horsepower <- as.numeric(Auto$horsepower)
Auto <- na.omit(Auto)
#quantitative:mpg; cylinders; displacement; horsepower;weight; acceleration;year; 
#qualitative:origin; name
#mpg
range(Auto[, 1])
#cylinders
range(Auto[, 2])
#displacement
range(Auto[, 3])
#horsepower
range(Auto[, 4])
#weight
range(Auto[, 5])
#acceleration
range(Auto[, 6])
#year
range(Auto[, 7])
```
(c) What is the mean and standard deviation of each quantitative predictor?
```{r}

sapply(Auto[, 1:7], mean)
sapply(Auto[, 1:7], sd)
```
(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?
```{r}
Auto_sub <- Auto[-(10:85), ]
sapply(Auto_sub[, 1:8], range)
sapply(Auto_sub[, 1:8], mean)
sapply(Auto_sub[, 1:8], sd)
```
(e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment
on your findings.
```{r}
pairs(Auto[, 1:8])#feel like name can't be plotted

```
(f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.
#displacement; horsepower;weight
```{r}
cor(Auto[,1:7])

Auto %>% 
  ggplot(aes(x = displacement, y = mpg)) + 
  geom_point(color = "blue")+
  geom_smooth(color = "green")

Auto %>% 
  ggplot(aes(x = weight, y = mpg)) + 
  geom_point(color = "blue")+
  geom_smooth(color = "green")

```


10. This exercise involves the Boston housing data set.
(a) To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library
How many rows are in this data set? How many columns? What do the rows and columns represent?
```{r}
Boston
# 506*13
#rows the size of the sample; cols the number of independent variable/predictors
```

(b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your fndings.
```{r}
pairs(Boston)
```

(c) Are any of the predictors associated with per capital crime rate? If so, explain the relationship.
```{r}
cor(Boston[,1:13])
#rad; tax
```

(d) Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.
```{r}
range(Boston$crim) #crime rates
range(Boston$tax) #Tax rates
range(Boston$ptratio)
```

(e) How many of the census tracts in this data set bound the Charles river?
```{r}
Boston %>% 
  count(chas == 1) #35
```

(f) What is the median pupil-teacher ratio among the towns in this data set?
```{r}
summary(Boston$ptratio) #19.05
```

(g) Which census tract of Boston has lowest median value of owner occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.
```{r}
Boston %>% 
  filter(medv == min(medv)) # tax rate; ptratio high in the range
range(Boston$crim) #crime rates
range(Boston$tax) #Tax rates
range(Boston$ptratio)
```

(h) In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling

```{r}
Boston %>% 
  filter(rm > 7) %>% 
  count() #64

Boston %>% 
  filter(rm > 8) %>% 
  count() #13 Census tracts with an average of more than eight rooms per dwelling are significantly fewer than those with an average of more than seven rooms per dwelling
```












###########labs
##basics
```{r }
x <- c(1,2,3) #vector
ls()# a list of all of the objects;like data and functions
rm(x)#delete
rm(list = ls())# remove list of all objects
```

##matrix 

```{r  echo = FALSE }
y <- matrix(data = c(2,3,2,6), nrow = 2, ncol = 2)
y <- matrix(c(2,3,2,6), 2, 2)#same with above
z <- matrix(c(2,3,2,6), 2, 2,byrow = TRUE)#populate the matrix in order of the rows
print(y)
print(z)
```

#The sqrt() function returns the square root of each element of a vector or matrix. The command x2̂raises each element of x sqrt() to the power 2
```{r}
sqrt(y)
y^2
```
#The rnorm() function generates a vector of random normal variables, with frst argument n the sample size.
#By default, rnorm() creates standard normal random variables with a mean of 0 and a standard deviation of 1.
```{r}
x <- rnorm(50)
y <- x + rnorm(50, mean = 50, sd = .1)
cor(x,y)

```


```{r}
set.seed(1008)
rnorm(50)
```
#The function dev.off() indicates to R that we are done creating the plot.
```{r}
x <- rnorm(100)
y <- rnorm(100)
plot(x,y)
```
#run the 3 lines together
```{r}
pdf("Figure.pdf")
plot(x, y, col = "green")

dev.off()





```

#The function seq() can be used to create a sequence of numbers.
#The contour() func- contour() tion produces a contour plot in order to represent three-dimensional data; contour plot it is like a topographical map
```{r}
x <- seq(1, 10)
y <- x

f <- outer(x, y, function(x, y) cos(y) / (1 + x^2))

           
contour(x, y, f)


contour(x, y, f, nlevels = 45)

fa <- (f - t(f)) / 2
contour(x, y, fa, nlevels = 15)
persp(x, y, fa)
persp(x, y, fa, theta = 20, phi = 10)
```


```{r}
A <- matrix(1:16,4,4)
A
A[c(1,3), c(2,4)]#get[1,2][1,4][3,2][3,4]
A[1:3, 2:4]
```
#attach(Auto) use the attach() function in attach() order to tell R to make the variables in this data frame available by name.
#The as.factor() function converts quantitative variables into qualitative as.factor() variables.
```{r}
rm(cylinders)
Auto <- read_csv("Auto.csv")
plot(Auto$cylinders,Auto$mpg)
attach(Auto)
plot(cylinders, mpg)
```


```{r}
cylinders <- as.factor(cylinders)
plot(cylinders, mpg)
plot(cylinders , mpg, col = "red", varwidth = T,horizontal = T,
     xlab = "cylinders", ylab = "MPG")
hist(mpg)

rm(cylinders)
Auto <- read_csv("Auto.csv")
attach(Auto)
Auto$horsepower <- as.numeric(horsepower)



pairs( ~ mpg + displacement + horsepower + weight + acceleration, data = Auto)
```
#run the two line of code together
```{r}
attach(Auto)

plot(horsepower, mpg)
identify(horsepower, mpg, labels=name)
```

```{r}
summary(Auto)
```

