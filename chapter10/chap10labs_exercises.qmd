---
title: "Untitled"
format: pdf
editor: visual
---

```{r}
library(ISLR2)
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n / 3)
testid <- sample(1:n, ntest)
library(glmnet)
library(torch)
library(luz) # high-level interface for torch
library(torchvision) # for datasets and image transformation
library(torchdatasets) # for datasets we are going to use
library(zeallot)
torch_manual_seed(13)
library(ggplot2)
```

###########labs #10.9.1 A Single Layer Network on the Hitters Data

```{r}
lfit <- lm(Salary ~ ., data = Gitters[-testid, ])
lpred <- predict(lfit, Gitters[testid, ])
with(Gitters[testid, ], mean(abs(lpred - Salary)))
```

# fit the lasso

```{r}
x <- scale(model.matrix(Salary ~ . - 1, data = Gitters))
y <- Gitters$Salary

cvfit <- cv.glmnet(x[-testid, ], y[-testid],
    type.measure = "mae")
cpred <- predict(cvfit, x[testid, ], s = "lambda.min")
mean(abs(y[testid] - cpred))
```

#to fit the neural network, we frst set up a model structure that describes the network.

# algorithm tracks the mean absolute error on the training data, and on validation data if it is supplied.

```{r}
modnn <- nn_module(
  initialize = function(input_size) {
    self$hidden <- nn_linear(input_size, 50)
    self$activation <- nn_relu()
    self$dropout <- nn_dropout(0.2)
    self$output <- nn_linear(50, 1)
  },
  forward = function(x) {
    x %>%
      self$hidden() %>%
      self$activation() %>%
      self$dropout() %>%
      self$output()
  }
)

x <- model.matrix(Salary ~ . - 1, data = Gitters) %>% scale()

modnn <- modnn %>%
  setup(
    loss = nn_mse_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_mae())
  ) %>%
  set_hparams(input_size = ncol(x))
```

```{r}
fitted <- modnn %>%
  fit(
    data = list(x[-testid, ], matrix(y[-testid], ncol = 1)),
    valid_data = list(x[testid, ], matrix(y[testid], ncol = 1)),
    epochs = 50
  )

plot(fitted)
```

```{r}
npred <- predict(fitted, x[testid, ])
npred_array <- as_array(npred)
mae <- mean(abs(y[testid] - npred_array))
print(mae)
```
`

#10.9.2 A Multilayer Network on the MNIST Digit Data

```{r}
library(ISLR2)
train_ds <- mnist_dataset(root = ".", train = TRUE, download = TRUE)
test_ds <- mnist_dataset(root = ".", train = FALSE, download = TRUE)

str(train_ds[1])
str(test_ds[2])

length(train_ds)
length(test_ds)

transform <- function(x) {
  x %>%
    torch_tensor() %>%
    torch_flatten() %>%
    torch_div(255)
}
train_ds <- mnist_dataset(
  root = ".",
  train = TRUE,
  download = TRUE,
  transform = transform
)
test_ds <- mnist_dataset(
  root = ".",
  train = FALSE,
  download = TRUE,
  transform = transform
)
```

```{r}
modelnn <- nn_module(
  initialize = function() {
    self$linear1 <- nn_linear(in_features = 28*28, out_features = 256)
    self$linear2 <- nn_linear(in_features = 256, out_features = 128)
    self$linear3 <- nn_linear(in_features = 128, out_features = 10)

    self$drop1 <- nn_dropout(p = 0.4)
    self$drop2 <- nn_dropout(p = 0.3)

    self$activation <- nn_relu()
  },
  forward = function(x) {
    x %>%

      self$linear1() %>%
      self$activation() %>%
      self$drop1() %>%

      self$linear2() %>%
      self$activation() %>%
      self$drop2() %>%

      self$linear3()
  }
)

print(modelnn())
```

```{r}
modelnn <- modelnn %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_accuracy())
  )

system.time(
   fitted <- modelnn %>%
      fit(
        data = train_ds,
        epochs = 5,
        valid_data = 0.2,
        dataloader_options = list(batch_size = 256),
        verbose = FALSE
      )
 )
plot(fitted)
```

```{r}
accuracy <- function(pred, truth) {
   mean(pred == truth) }

# gets the true classes from all observations in test_ds.
truth <- sapply(seq_along(test_ds), function(x) test_ds[x][[2]])

fitted %>%
  predict(test_ds) %>%
  torch_argmax(dim = 2) %>%  # the predicted class is the one with higher 'logit'.
  as_array() %>% # we convert to an R object
  accuracy(truth)

modellr <- nn_module(
  initialize = function() {
    self$linear <- nn_linear(784, 10)
  },
  forward = function(x) {
    self$linear(x)
  }
)
print(modellr())
```

```{r}
fit_modellr <- modellr %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_accuracy())
  ) %>%
  fit(
    data = train_ds,
    epochs = 5,
    valid_data = 0.2,
    dataloader_options = list(batch_size = 128)
  )

fit_modellr %>%
  predict(test_ds) %>%
  torch_argmax(dim = 2) %>%  # the predicted class is the one with higher 'logit'.
  as_array() %>% # we convert to an R object
  accuracy(truth)


# alternatively one can use the `evaluate` function to get the results
# on the test_ds
evaluate(fit_modellr, test_ds)
```

#10.9.3 Convolutional Neural Networks

```{r}
transform <- function(x) {
  transform_to_tensor(x)
}

train_ds <- cifar100_dataset(
  root = "./",
  train = TRUE,
  download = TRUE,
  transform = transform
)

test_ds <- cifar100_dataset(
  root = "./",
  train = FALSE,
  transform = transform
)

str(train_ds[1])
length(train_ds)
```

```{r}
par(mar = c(0, 0, 0, 0), mfrow = c(5, 5))
index <- sample(seq(50000), 25)
for (i in index) plot(as.raster(as.array(train_ds[i][[1]]$permute(c(2,3,1)))))
```

```{r}
conv_block <- nn_module(
  initialize = function(in_channels, out_channels) {
    self$conv <- nn_conv2d(
      in_channels = in_channels,
      out_channels = out_channels,
      kernel_size = c(3,3),
      padding = "same"
    )
    self$relu <- nn_relu()
    self$pool <- nn_max_pool2d(kernel_size = c(2,2))
  },
  forward = function(x) {
    x %>%
      self$conv() %>%
      self$relu() %>%
      self$pool()
  }
)

model <- nn_module(
  initialize = function() {
    self$conv <- nn_sequential(
      conv_block(3, 32),
      conv_block(32, 64),
      conv_block(64, 128),
      conv_block(128, 256)
    )
    self$output <- nn_sequential(
      nn_dropout(0.5),
      nn_linear(2*2*256, 512),
      nn_relu(),
      nn_linear(512, 100)
    )
  },
  forward = function(x) {
    x %>%
      self$conv() %>%
      torch_flatten(start_dim = 2) %>%
      self$output()
  }
)
model()
```

```{r}
fitted <- model %>%
  setup(
    loss = nn_cross_entropy_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_accuracy())
  ) %>%
  set_opt_hparams(lr = 0.001) %>%
  fit(
    train_ds,
    epochs = 10, #30,
    valid_data = 0.2,
    dataloader_options = list(batch_size = 128)
  )

print(fitted)

evaluate(fitted, test_ds)
```

#10.9.4 Using Pretrained CNN Models

```{r}
img_dir <- "book_images"
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- torch_empty(num_images, 3, 224, 224)
for (i in 1:num_images) {
   img_path <- file.path(img_dir, image_names[i])
   img <- img_path %>%
     base_loader() %>%
     transform_to_tensor() %>%
     transform_resize(c(224, 224)) %>%
     # normalize with imagenet mean and stds.
     transform_normalize(
       mean = c(0.485, 0.456, 0.406),
       std = c(0.229, 0.224, 0.225)
     )
   x[i,,, ] <- img
}
```

```{r}
model <- torchvision::model_resnet18(pretrained = TRUE)
model$eval() # put the model in evaluation mode
preds <- model(x)

mapping <- jsonlite::read_json("https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json") %>%
  sapply(function(x) x[[2]])

top3 <- torch_topk(preds, dim = 2, k = 3)

top3_prob <- top3[[1]] %>%
  nnf_softmax(dim = 2) %>%
  torch_unbind() %>%
  lapply(as.numeric)

top3_class <- top3[[2]] %>%
  torch_unbind() %>%
  lapply(function(x) mapping[as.integer(x)])

result <- purrr::map2(top3_prob, top3_class, function(pr, cl) {
  names(pr) <- cl
  pr
})
names(result) <- image_names
print(result)
```

#10.9.5 IMDb Document Classifcation

```{r}
max_features <- 10000
imdb_train <- imdb_dataset(
  root = ".",
  download = TRUE,
  num_words = max_features
)
imdb_test <- imdb_dataset(
  root = ".",
  download = TRUE,
  num_words = max_features
)

imdb_train[1]$x[1:12]
```

```{r}
word_index <- imdb_train$vocabulary
decode_review <- function(text, word_index) {
   word <- names(word_index)
   idx <- unlist(word_index, use.names = FALSE)
   word <- c("<PAD>", "<START>", "<UNK>", word)
   words <- word[text]
   paste(words, collapse = " ")
}
decode_review(imdb_train[1]$x[1:12], word_index)
```

```{r}
library(Matrix)
one_hot <- function(sequences, dimension) {
   seqlen <- sapply(sequences, length)
   n <- length(seqlen)
   rowind <- rep(1:n, seqlen)
   colind <- unlist(sequences)
   sparseMatrix(i = rowind, j = colind,
      dims = c(n, dimension))
}

# collect all values into a list
train <- seq_along(imdb_train) %>%
  lapply(function(i) imdb_train[i]) %>%
  purrr::transpose()
test <- seq_along(imdb_test) %>%
  lapply(function(i) imdb_test[i]) %>%
  purrr::transpose()

# num_words + padding + start + oov token = 10000 + 3
x_train_1h <- one_hot(train$x, 10000 + 3)
x_test_1h <- one_hot(test$x, 10000 + 3)
dim(x_train_1h)
nnzero(x_train_1h) / (25000 * (10000 + 3))
```

```{r}
set.seed(3)
ival <- sample(seq(along = train$y), 2000)
itrain <- seq_along(train$y)[-ival]

library(glmnet)
y_train <- unlist(train$y)

fitlm <- glmnet(x_train_1h[itrain, ], unlist(y_train[itrain]),
    family = "binomial", standardize = FALSE)
classlmv <- predict(fitlm, x_train_1h[ival, ]) > 0
acclmv <- apply(classlmv, 2, accuracy,  unlist(y_train[ival]) > 0)

par(mar = c(4, 4, 4, 4), mfrow = c(1, 1))
plot(-log(fitlm$lambda), acclmv)
```

```{r}
model <- nn_module(
  initialize = function(input_size = 10000 + 3) {
    self$dense1 <- nn_linear(input_size, 16)
    self$relu <- nn_relu()
    self$dense2 <- nn_linear(16, 16)
    self$output <- nn_linear(16, 1)
  },
  forward = function(x) {
    x %>%
      self$dense1() %>%
      self$relu() %>%
      self$dense2() %>%
      self$relu() %>%
      self$output() %>%
      torch_flatten(start_dim = 1)
  }
)
model <- model %>%
  setup(
    loss = nn_bce_with_logits_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_binary_accuracy_with_logits())
  ) %>%
  set_opt_hparams(lr = 0.001)

fitted <- model %>%
  fit(
    # we transform the training and validation data into torch tensors
    list(
      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()),
      torch_tensor(unlist(train$y[itrain]))
    ),
    valid_data = list(
      torch_tensor(as.matrix(x_train_1h[ival, ]), dtype = torch_float()),
      torch_tensor(unlist(train$y[ival]))
    ),
    dataloader_options = list(batch_size = 512),
    epochs = 10
  )

plot(fitted)
```

```{r}
fitted <- model %>%
  fit(
    list(
      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()),
      torch_tensor(unlist(train$y[itrain]))
    ),
    valid_data = list(
      torch_tensor(as.matrix(x_test_1h), dtype = torch_float()),
      torch_tensor(unlist(test$y))
    ),
    dataloader_options = list(batch_size = 512),
    epochs = 10
  )
```

#10.9.6 Recurrent Neural Networks \## Sequential Models for Document Classification

```{r}
wc <- sapply(seq_along(imdb_train), function(i) length(imdb_train[i]$x))
median(wc)
sum(wc <= 500) / length(wc)
maxlen <- 500
num_words <- 10000
imdb_train <- imdb_dataset(root = ".", split = "train", num_words = num_words,
                           maxlen = maxlen)
imdb_test <- imdb_dataset(root = ".", split = "test", num_words = num_words,
                           maxlen = maxlen)

vocab <- c(rep(NA, imdb_train$index_from - 1), imdb_train$get_vocabulary())
tail(names(vocab)[imdb_train[1]$x])
```

```{r}
model <- nn_module(
  initialize = function() {
    self$embedding <- nn_embedding(10000 + 3, 32)
    self$lstm <- nn_lstm(input_size = 32, hidden_size = 32, batch_first = TRUE)
    self$dense <- nn_linear(32, 1)
  },
  forward = function(x) {
    c(output, c(hn, cn)) %<-% (x %>%
      self$embedding() %>%
      self$lstm())
    output[,-1,] %>%  # get the last output
      self$dense() %>%
      torch_flatten(start_dim = 1)
  }
)

model <- model %>%
  setup(
    loss = nn_bce_with_logits_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_binary_accuracy_with_logits())
  ) %>%
  set_opt_hparams(lr = 0.001)

fitted <- model %>% fit(
  imdb_train,
  epochs = 10,
  dataloader_options = list(batch_size = 128),
  valid_data = imdb_test
)
plot(fitted)
predy <- torch_sigmoid(predict(fitted, imdb_test)) > 0.5
evaluate(fitted, imdb_test, dataloader_options = list(batch_size = 512))
```

##  Time Series Prediction

```{r}
library(ISLR2)
xdata <- data.matrix(
 NYSE[, c("DJ_return", "log_volume","log_volatility")]
 )
istrain <- NYSE[, "train"]
xdata <- scale(xdata)

lagm <- function(x, k = 1) {
   n <- nrow(x)
   pad <- matrix(NA, k, ncol(x))
   rbind(pad, x[1:(n - k), ])
}

arframe <- data.frame(log_volume = xdata[, "log_volume"],
   L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
   L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
   L5 = lagm(xdata, 5)
 )

arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]

arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
1 - mean((arpred - arframe[!istrain, "log_volume"])^2) / V0

```


```{r}
arframed <-
    data.frame(day = NYSE[-(1:5), "day_of_week"], arframe)
arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])^2) / V0
```


```{r}
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
dim(xrnn)

model <- nn_module(
  initialize = function() {
    self$rnn <- nn_rnn(3, 12, batch_first = TRUE)
    self$dense <- nn_linear(12, 1)
    self$dropout <- nn_dropout(0.2)
  },
  forward = function(x) {
    c(output, ...) %<-% (x %>%
      self$rnn())
    output[,-1,] %>%
      self$dropout() %>%
      self$dense() %>%
      torch_flatten(start_dim = 1)
  }
)

model <- model %>%
  setup(
    optimizer = optim_rmsprop,
    loss = nn_mse_loss()
  ) %>%
  set_opt_hparams(lr = 0.001)


```


```{r}
fitted <- model %>% fit(
    list(xrnn[istrain,, ], arframe[istrain, "log_volume"]),
    epochs = 75, #epochs = 200,
    dataloader_options = list(batch_size = 64),
    valid_data =
      list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"])
  )
kpred <- as.numeric(predict(fitted, xrnn[!istrain,, ]))
1 - mean((kpred - arframe[!istrain, "log_volume"])^2) / V0
```

```{r}
model <- nn_module(
  initialize = function() {
    self$dense <- nn_linear(15, 1)
  },
  forward = function(x) {
    x %>%
      torch_flatten(start_dim = 2) %>%
      self$dense()
  }
)
```

```{r}
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
```

```{r}
arnnd <- nn_module(
  initialize = function() {
    self$dense <- nn_linear(15, 32)
    self$dropout <- nn_dropout(0.5)
    self$activation <- nn_relu()
    self$output <- nn_linear(32, 1)

  },
  forward = function(x) {
    x %>%
      torch_flatten(start_dim = 2) %>%
      self$dense() %>%
      self$activation() %>%
      self$dropout() %>%
      self$output() %>%
      torch_flatten(start_dim = 1)
  }
)
arnnd <- arnnd %>%
  setup(
    optimizer = optim_rmsprop,
    loss = nn_mse_loss()
  ) %>%
  set_opt_hparams(lr = 0.001)

fitted <- arnnd %>% fit(
    list(xrnn[istrain,, ], arframe[istrain, "log_volume"]),
    epochs = 30, #epochs = 200,
    dataloader_options = list(batch_size = 64),
    valid_data =
      list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"])
  )
plot(fitted)
npred <- as.numeric(predict(fitted, xrnn[!istrain, ,]))
1 - mean((arframe[!istrain, "log_volume"] - npred)^2) / V0
```

#6.
```{r}
R_beta <- function(beta) {
  sin(beta) + beta / 10
}

b <- seq(-6, 6, length.out = 400)

R_b <- R_beta(b)

plot(b, R_b, type = "l", col = "blue", lwd = 2,
     xlab = expression(beta), ylab = expression(R(beta)))
```
```{r}
dR_beta <- function(beta) {
  cos(beta) + 0.1
}

beta_0 <- 2.3
learning_rate <- 0.1
num_iterations <- 100

beta_values <- numeric(num_iterations)
beta_values[1] <- beta_0

for (t in 2:num_iterations) {
  beta_values[t] <- beta_values[t-1] - learning_rate * dR_beta(beta_values[t-1])
}

plot(b, R_b, type = "l", col = "blue", lwd = 2, xlab = expression(beta), ylab = expression(R(beta)))
points(beta_values, R_beta(beta_values), col = "red", pch = 19)

cat("Final β value after gradient descent:", beta_values[num_iterations], "\n")

```
```{r}
beta_1 <- 1.4
beta_values <- numeric(num_iterations)
beta_values[1] <- beta_1

for (t in 2:num_iterations) {
  beta_values[t] <- beta_values[t-1] - learning_rate * dR_beta(beta_values[t-1])
}

plot(b, R_b, type = "l", col = "blue", lwd = 2, xlab = expression(beta), ylab = expression(R(beta)))
points(beta_values, R_beta(beta_values), col = "green", pch = 19)
cat("Final β value after gradient descent:", beta_values[num_iterations], "\n")
```
#7.Fit a neural network to the Default data.
```{r}
Default$default <- ifelse(Default$default == "Yes", 1, 0)

n <- nrow(Default)
set.seed(13)
ntest <- trunc(n / 5)
testid <- sample(1:n, ntest)

modnn <- nn_module(
  initialize = function(input_size) {
    self$hidden <- nn_linear(input_size, 10)
    self$activation <- nn_relu()
    self$dropout <- nn_dropout(0.3)
    self$output <- nn_linear(10, 1)
  },
  forward = function(x) {
    x %>%
      self$hidden() %>%
      self$activation() %>%
      self$dropout() %>%
      self$output()
  }
)

x <- model.matrix(default ~ . - 1, data = Default) %>% scale()
y <- Default$default

modnn <- modnn %>%
  setup(
    loss = nn_bce_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_mae())
  ) %>%
  set_hparams(input_size = ncol(x))
```

```{r}
fitted <- modnn %>%
  fit(
    data = list(x[-testid, ], matrix(y[-testid], ncol = 1)),
    valid_data = list(x[testid, ], matrix(y[testid], ncol = 1)),
    epochs = 50
  )

plot(fitted)
```

```{r}
npred <- predict(fitted, x[testid, ])
npred_array <- as_array(npred)
predictions <- ifelse(npred_array > 0.5, 1, 0)
accuracy <- mean(predictions == y[testid])

print(paste("Accuracy:", accuracy))
```

#8. From your collection of personal photographs, pick 10 images of animals

```{r}
img_dir <- "animal_images"
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- torch_empty(num_images, 3, 224, 224)
for (i in 1:num_images) {
   img_path <- file.path(img_dir, image_names[i])
   img <- img_path %>%
     base_loader() %>%
     transform_to_tensor() %>%
     transform_resize(c(224, 224)) %>%
     # normalize with imagenet mean and stds.
     transform_normalize(
       mean = c(0.485, 0.456, 0.406),
       std = c(0.229, 0.224, 0.225)
     )
   x[i,,, ] <- img
}
```

```{r}
model <- torchvision::model_resnet18(pretrained = TRUE)
model$eval() # put the model in evaluation mode
preds <- model(x)

mapping <- jsonlite::read_json("https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json") %>%
  sapply(function(x) x[[2]])

top5 <- torch_topk(preds, dim = 2, k = 5)

top5_prob <- top5[[1]] %>%
  nnf_softmax(dim = 2) %>%
  torch_unbind() %>%
  lapply(as.numeric)

top5_class <- top5[[2]] %>%
  torch_unbind() %>%
  lapply(function(x) mapping[as.integer(x)])

result <- purrr::map2(top5_prob, top5_class, function(pr, cl) {
  names(pr) <- cl
  pr
})
names(result) <- image_names
print(result)
```

#9. Fit a lag-5 autoregressive model to the NYSE data

```{r}
library(ISLR2)
xdata <- data.matrix(
 NYSE[, c("DJ_return", "log_volume","log_volatility")]
 )
istrain <- NYSE[, "train"]
xdata <- scale(xdata)

lagm <- function(x, k = 1) {
   n <- nrow(x)
   pad <- matrix(NA, k, ncol(x))
   rbind(pad, x[1:(n - k), ])
}

arframe <- data.frame(log_volume = xdata[, "log_volume"],
   L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
   L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
   L5 = lagm(xdata, 5)
 )

arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
V0 <- var(arframe[!istrain, "log_volume"])
```

```{r}
NYSE$month <- format(as.Date(NYSE$date), "%m")
NYSE$month <- factor(NYSE$month, levels = sprintf("%02d", 1:12))
arframed <- data.frame(day = NYSE[-(1:5), "day_of_week"], month = NYSE[-(1:5), "month"], arframe)

arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])^2) / V0
```
#0.4629872


#10.
```{r}
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
dim(xrnn)

model <- nn_module(
  initialize = function() {
    self$rnn <- nn_rnn(3, 12, batch_first = TRUE)
    self$dense <- nn_linear(12, 1)
    self$dropout <- nn_dropout(0.2)
  },
  forward = function(x) {
    c(output, ...) %<-% (x %>%
      self$rnn())
    output[,-1,] %>%
      self$dropout() %>%
      self$dense() %>%
      torch_flatten(start_dim = 1)
  }
)

model <- model %>%
  setup(
    optimizer = optim_rmsprop,
    loss = nn_mse_loss()
  ) %>%
  set_opt_hparams(lr = 0.001)

```

```{r}
fitted <- model %>% fit(
    list(xrnn[istrain,, ], arframe[istrain, "log_volume"]),
    epochs = 200, #epochs = 200,
    dataloader_options = list(batch_size = 64),
    valid_data =
      list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"])
  )
kpred <- as.numeric(predict(fitted, xrnn[!istrain,, ]))
1 - mean((kpred - arframe[!istrain, "log_volume"])^2) / V0
```

#11.

```{r}
model <- nn_module(
  initialize = function() {
    self$dense <- nn_linear(15, 1)
  },
  forward = function(x) {
    x %>%
      torch_flatten(start_dim = 2) %>%
      self$dense()
  }
)
```

```{r}
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
```




```{r}
arnnd <- nn_module(
  initialize = function() {
    self$dense <- nn_linear(15, 32)
    self$dropout <- nn_dropout(0.5)
    self$activation <- nn_relu()
    self$output <- nn_linear(32, 1)

  },
  forward = function(x) {
    x %>%
      torch_flatten(start_dim = 2) %>%
      self$dense() %>%
      self$activation() %>%
      self$dropout() %>%
      self$output() %>%
      torch_flatten(start_dim = 1)
  }
)
arnnd <- arnnd %>%
  setup(
    optimizer = optim_rmsprop,
    loss = nn_mse_loss()
  ) %>%
  set_opt_hparams(lr = 0.001)

```




```{r}
fitted <- arnnd %>% fit(
    list(xrnn[istrain,, ], arframe[istrain, "log_volume"]),
    epochs = 30,
    dataloader_options = list(batch_size = 64),
    valid_data =
      list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"])
  )
plot(fitted)
npred <- as.numeric(predict(fitted, xrnn[!istrain, ,]))
1 - mean((arframe[!istrain, "log_volume"] - npred)^2) / V0
```
#13.

```{r}
dict_sizes <- c(1000, 3000, 5000, 10000)

accuracy <- function(pred, truth) {
   mean(pred == truth) }

# Loop through each dictionary size
for (max_features in dict_sizes) {
  
  cat("\n\nTesting with dictionary size:", max_features, "\n\n")
  
  imdb_train <- imdb_dataset(
    root = ".",
    download = TRUE,
    num_words = max_features
  )
  
  imdb_test <- imdb_dataset(
    root = ".",
    download = TRUE,
    num_words = max_features
  )
  

  word_index <- imdb_train$vocabulary
  
  decode_review <- function(text, word_index) {
    word <- names(word_index)
    idx <- unlist(word_index, use.names = FALSE)
    word <- c("<PAD>", "<START>", "<UNK>", word)
    words <- word[text]
    paste(words, collapse = " ")
  }
  
  print(decode_review(imdb_train[1]$x[1:12], word_index))
  
  one_hot <- function(sequences, dimension) {
    seqlen <- sapply(sequences, length)
    n <- length(seqlen)
    rowind <- rep(1:n, seqlen)
    colind <- unlist(sequences)
    sparseMatrix(i = rowind, j = colind, dims = c(n, dimension))
  }
  
  train <- seq_along(imdb_train) %>% 
    lapply(function(i) imdb_train[i]) %>% 
    purrr::transpose()
  
  test <- seq_along(imdb_test) %>% 
    lapply(function(i) imdb_test[i]) %>% 
    purrr::transpose()
  
  # One-hot encoding (adjust the size according to max_features)
  x_train_1h <- one_hot(train$x, max_features + 3)  # Padding and special tokens
  x_test_1h <- one_hot(test$x, max_features + 3)
  
  cat("Train data dimensions: ", dim(x_train_1h), "\n")
  cat("Non-zero elements in train set: ", nnzero(x_train_1h) / (25000 * (max_features + 3)), "\n")
  

  set.seed(3)
  ival <- sample(seq(along = train$y), 2000)  # Validation set indices
  itrain <- seq_along(train$y)[-ival]  # Training set indices
  
  y_train <- unlist(train$y)
  
  fitlm <- glmnet(x_train_1h[itrain, ], unlist(y_train[itrain]),
                  family = "binomial", standardize = FALSE)
  
  classlmv <- predict(fitlm, x_train_1h[ival, ]) > 0
  acclmv <- apply(classlmv, 2, accuracy,  unlist(y_train[ival]) > 0)
  
  

  model <- nn_module(
    initialize = function(input_size = max_features + 3) {
      self$dense1 <- nn_linear(input_size, 16)
      self$relu <- nn_relu()
      self$dense2 <- nn_linear(16, 16)
      self$output <- nn_linear(16, 1)
    },
    forward = function(x) {
      x %>%
        self$dense1() %>%
        self$relu() %>%
        self$dense2() %>%
        self$relu() %>%
        self$output() %>%
        torch_flatten(start_dim = 1)
    }
  )
  
  model <- model %>%
    setup(
      loss = nn_bce_with_logits_loss(),
      optimizer = optim_rmsprop,
      metrics = list(luz_metric_binary_accuracy_with_logits())
    ) %>%
    set_opt_hparams(lr = 0.001)
  
  fitted <- model %>%
    fit(
      list(
        torch_tensor(as.matrix(x_train_1h[itrain, ]), dtype = torch_float()),
        torch_tensor(unlist(train$y[itrain]))
      ),
      valid_data = list(
        torch_tensor(as.matrix(x_train_1h[ival, ]), dtype = torch_float()),
        torch_tensor(unlist(train$y[ival]))
      ),
      dataloader_options = list(batch_size = 512),
      epochs = 10
    )
  

  fitted <- model %>%
    fit(
      list(
        torch_tensor(as.matrix(x_train_1h[itrain, ]), dtype = torch_float()),
        torch_tensor(unlist(train$y[itrain]))
      ),
      valid_data = list(
        torch_tensor(as.matrix(x_test_1h), dtype = torch_float()),
        torch_tensor(unlist(test$y))
      ),
      dataloader_options = list(batch_size = 512),
      epochs = 10
    )
  
  cat("\nFinished testing with dictionary size:", max_features, "\n\n")
}

```

